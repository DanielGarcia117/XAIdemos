{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# %% Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pandas as pd \n",
    "\n",
    "# Set GPU device\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/daniel/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc2ab264d9e416d82dd16dfa8f7cfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (vgg16): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Building the model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.vgg16 = models.vgg16(pretrained=True) \n",
    "\n",
    "        # Replace output layer according to our problem\n",
    "        in_feats = self.vgg16.classifier[6].in_features \n",
    "        self.vgg16.classifier[6] = nn.Linear(in_feats, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg16(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_ROOT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-05d05e1ca5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %% Prepare data for pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataset = torchvision.datasets.ImageFolder(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         transform=transforms.Compose([\n\u001b[1;32m      5\u001b[0m                       \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAIN_ROOT' is not defined"
     ]
    }
   ],
   "source": [
    "# %% Prepare data for pretrained model\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TRAIN_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TEST_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "#train_dataset[0][0].permute(1,2,0)\n",
    "\n",
    "# %% Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Train\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "epochs = 10\n",
    "\n",
    "# Iterate x epochs over the train data\n",
    "for epoch in range(epochs):  \n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # Labels are automatically one-hot-encoded\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Inspect predictions for first batch\n",
    "import pandas as pd\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.numpy()\n",
    "outputs = model(inputs).max(1).indices.detach().cpu().numpy()\n",
    "comparison = pd.DataFrame()\n",
    "print(\"Batch accuracy: \", (labels==outputs).sum()/len(labels))\n",
    "comparison[\"labels\"] = labels\n",
    "\n",
    "comparison[\"outputs\"] = outputs\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Layerwise relevance propagation for VGG16\n",
    "# For other CNN architectures this code might become more complex\n",
    "# Source: https://git.tu-berlin.de/gmontavon/lrp-tutorial\n",
    "# http://iphome.hhi.de/samek/pdf/MonXAI19.pdf\n",
    "\n",
    "def new_layer(layer, g):\n",
    "    \"\"\"Clone a layer and pass its parameters through the function g.\"\"\"\n",
    "    layer = copy.deepcopy(layer)\n",
    "    try: layer.weight = torch.nn.Parameter(g(layer.weight))\n",
    "    except AttributeError: pass\n",
    "    try: layer.bias = torch.nn.Parameter(g(layer.bias))\n",
    "    except AttributeError: pass\n",
    "    return layer\n",
    "\n",
    "def dense_to_conv(layers):\n",
    "    \"\"\" Converts a dense layer to a conv layer \"\"\"\n",
    "    newlayers = []\n",
    "    for i,layer in enumerate(layers):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            newlayer = None\n",
    "            if i == 0:\n",
    "                m, n = 512, layer.weight.shape[0]\n",
    "                newlayer = nn.Conv2d(m,n,7)\n",
    "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,7,7))\n",
    "            else:\n",
    "                m,n = layer.weight.shape[1],layer.weight.shape[0]\n",
    "                newlayer = nn.Conv2d(m,n,1)\n",
    "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,1,1))\n",
    "            newlayer.bias = nn.Parameter(layer.bias)\n",
    "            newlayers += [newlayer]\n",
    "        else:\n",
    "            newlayers += [layer]\n",
    "    return newlayers\n",
    "\n",
    "def get_linear_layer_indices(model):\n",
    "    offset = len(model.vgg16._modules['features']) + 1\n",
    "    indices = []\n",
    "    for i, layer in enumerate(model.vgg16._modules['classifier']): \n",
    "        if isinstance(layer, nn.Linear): \n",
    "            indices.append(i)\n",
    "    indices = [offset + val for val in indices]\n",
    "    return indices\n",
    "\n",
    "def apply_lrp_on_vgg16(model, image):\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    # >>> Step 1: Extract layers\n",
    "    layers = list(model.vgg16._modules['features']) \\\n",
    "                + [model.vgg16._modules['avgpool']] \\\n",
    "                + dense_to_conv(list(model.vgg16._modules['classifier']))\n",
    "    linear_layer_indices = get_linear_layer_indices(model)\n",
    "    # >>> Step 2: Propagate image through layers and store activations\n",
    "    n_layers = len(layers)\n",
    "    activations = [image] + [None] * n_layers # list of activations\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        if layer in linear_layer_indices:\n",
    "            if layer == 32:\n",
    "                activations[layer] = activations[layer].reshape((1, 512, 7, 7))\n",
    "        activation = layers[layer].forward(activations[layer])\n",
    "        if isinstance(layers[layer], torch.nn.modules.pooling.AdaptiveAvgPool2d):\n",
    "            activation = torch.flatten(activation, start_dim=1)\n",
    "        activations[layer+1] = activation\n",
    "\n",
    "    # >>> Step 3: Replace last layer with one-hot-encoding\n",
    "    output_activation = activations[-1].detach().cpu().numpy()\n",
    "    max_activation = output_activation.max()\n",
    "    one_hot_output = [val if val == max_activation else 0 \n",
    "                        for val in output_activation[0]]\n",
    "\n",
    "    activations[-1] = torch.FloatTensor([one_hot_output]).to(device)\n",
    "\n",
    "    # >>> Step 4: Backpropagate relevance scores\n",
    "    relevances = [None] * n_layers + [activations[-1]]\n",
    "    # Iterate over the layers in reverse order\n",
    "    for layer in range(0, n_layers)[::-1]:\n",
    "        current = layers[layer]\n",
    "        # Treat max pooling layers as avg pooling\n",
    "        if isinstance(current, torch.nn.MaxPool2d):\n",
    "            layers[layer] = torch.nn.AvgPool2d(2)\n",
    "            current = layers[layer]\n",
    "        if isinstance(current, torch.nn.Conv2d) or \\\n",
    "           isinstance(current, torch.nn.AvgPool2d) or\\\n",
    "           isinstance(current, torch.nn.Linear):\n",
    "            activations[layer] = activations[layer].data.requires_grad_(True)\n",
    "            \n",
    "            # Apply variants of LRP depending on the depth\n",
    "            # see: https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10\n",
    "            # Lower layers, LRP-gamma >> Favor positive contributions (activations)\n",
    "            if layer <= 16:       rho = lambda p: p + 0.25*p.clamp(min=0); incr = lambda z: z+1e-9\n",
    "            # Middle layers, LRP-epsilon >> Remove some noise / Only most salient factors survive\n",
    "            if 17 <= layer <= 30: rho = lambda p: p;                       incr = lambda z: z+1e-9+0.25*((z**2).mean()**.5).data\n",
    "            # Upper Layers, LRP-0 >> Basic rule\n",
    "            if layer >= 31:       rho = lambda p: p;                       incr = lambda z: z+1e-9\n",
    "            \n",
    "            # Transform weights of layer and execute forward pass\n",
    "            z = incr(new_layer(layers[layer],rho).forward(activations[layer]))\n",
    "            # Element-wise division between relevance of the next layer and z\n",
    "            s = (relevances[layer+1]/z).data                                     \n",
    "            # Calculate the gradient and multiply it by the activation\n",
    "            (z * s).sum().backward(); \n",
    "            c = activations[layer].grad       \n",
    "            # Assign new relevance values           \n",
    "            relevances[layer] = (activations[layer]*c).data                          \n",
    "        else:\n",
    "            relevances[layer] = relevances[layer+1]\n",
    "\n",
    "    # >>> Potential Step 5: Apply different propagation rule for pixels\n",
    "    return relevances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Calculate relevances for first image in this test batch\n",
    "image_id = 31\n",
    "image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])\n",
    "image_relevances = image_relevances.permute(0,2,3,1).detach().cpu().numpy()[0]\n",
    "image_relevances = np.interp(image_relevances, (image_relevances.min(),\n",
    "                                                image_relevances.max()), \n",
    "                                                (0, 1))\n",
    "# Show relevances\n",
    "pred_label = list(test_dataset.class_to_idx.keys())[\n",
    "             list(test_dataset.class_to_idx.values())\n",
    "            .index(labels[image_id])]\n",
    "if outputs[image_id] == labels[image_id]:\n",
    "    print(\"Groundtruth for this image: \", pred_label)\n",
    "\n",
    "    # Plot images next to each other\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image_relevances[:,:,0], cmap=\"seismic\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(inputs[image_id].permute(1,2,0).detach().cpu().numpy())\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"This image is not classified correctly.\")\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
